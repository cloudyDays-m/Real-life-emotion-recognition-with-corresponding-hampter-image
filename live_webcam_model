import tensorflow as tf
import cv2
import os
import matplotlib.pyplot as plt
import numpy as np

new_model = tf.keras.models.load_model('my_model_64p35.h5')
path = "haarcascade_frontalface_default.xml"

font_scale = 1.5 

font = cv2.FONT_HERSHEY_PLAIN


rectangle_bgr = (255, 255, 255)

# img = np.zeros((500,500))

text = "Some text in a box"


(text_width, text_height) = cv2.getTextSize(text, font, fontScale=font_scale, thickness = 1)[0] 

# set the text to the start position 

text_offset_x = 10 
text_offset_y = 400 

box_coords = ((text_offset_x, text_offset_y), (text_offset_x + text_width + 2, text_offset_y - text_height - 2))
# cv2.rectangle(img, box_coords[0], box_coords[1], rectangle_bgr, cv2.FILLED)
# cv2.putText(img, text, (text_offset_x, text_offset_y), font, fontScale= font_scale, color=(0,0,0), thickness=1)

cap = cv2.VideoCapture(0)  # Try 0 first (usually the default webcam)

# checking if the webcam has opened correctly 
if not cap.isOpened():
    cap = cv2.VideoCapture(1)
if not cap.isOpened():
    raise IOError("Cannot Open Webcam")

while True: 
    ret, frame = cap.read()
    
    if not ret:  # Check if frame was read successfully
        print("Failed to grab frame")
        break
    
    faceCascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')
    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
    
    faces = faceCascade.detectMultiScale(gray, 1.1, 4)
    
    for (x, y, w, h) in faces:
        roi_gray = gray[y:y+h, x:x+w]
        roi_color = frame[y:y+h, x:x+w]
        
        facess = faceCascade.detectMultiScale(roi_gray)

        if len(facess) == 0:
            print("Face not detected")
        else:
            for (ex, ey, ew, eh) in facess:
                face_roi = roi_color[ey: ey+eh, ex: ex + ew]

            # Process the face
            final_image = cv2.resize(face_roi, (224, 224))
            final_image = np.expand_dims(final_image, axis=0)
            final_image = final_image / 255.0

            # Get predictions
            Predictions = new_model.predict(final_image, verbose=0)  # Added verbose=0 to reduce console spam
            emotion_index = np.argmax(Predictions)
            
            # Emotion labels
            emotions = ["Angry", "Happy", "Neutral", "Sad", "Surprised"]
            status = emotions[emotion_index]
            
            # Color coding for emotions
            colors = {
                "Angry": (0, 0, 255),      # Red
                "Happy": (0, 255, 0),      # Green
                "Neutral": (255, 255, 0),  # Cyan
                "Sad": (255, 0, 0),        # Blue
                "Surprised": (0, 165, 255) # Orange
            }
            color = colors[status]
            
            # Draw rectangle around face
            cv2.rectangle(frame, (x, y), (x+w, y+h), color, 3)
            
            # Draw black background box for text
            x1, y1, w1, h1 = 10, 10, 200, 50
            cv2.rectangle(frame, (x1, y1), (x1 + w1, y1 + h1), (0, 0, 0), -1)
            
            # Put text on frame
            cv2.putText(frame, status, (x1 + 10, y1 + 35), 
                       cv2.FONT_HERSHEY_SIMPLEX, 1.2, color, 2, cv2.LINE_AA)

    cv2.imshow('Emotion Detection', frame)
    
    # Press 'q' to quit
    if cv2.waitKey(1) & 0xFF == ord('q'):
        break

cap.release()
cv2.destroyAllWindows()